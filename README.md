# cve-intelligence-rag

This repository contains a CVE Query System built using Streamlit, which leverages Pinecone for vector database management and integrates powerful models like `all-mpnet-base-v2` for embeddings and `llama3-8b-8192` for generating responses. The application is deployed using GroqCloud for scalable compute.

## Features

- **CVE Query System**: Input a CVE identifier or description and retrieve relevant CVEs from the database.
- **Vector Database**: Stores embeddings of CVEs using Pinecone for fast and efficient querying.
- **Embeddings Model**: Uses `all-mpnet-base-v2` for generating embeddings of the input queries and CVE descriptions.
- **LLM Response Generation**: Generates human-readable responses using the `llama3-8b-8192` model, enhanced with the context of relevant neighbors from the database.
- **Scalable Deployment**: Powered by GroqCloud for efficient and scalable computation.

## Getting Started

### Prerequisites

- Python 3.8 or higher
- Streamlit
- Pinecone Python client
- GroqCloud SDK
- Sentence Transformers
- LLaMA model dependencies

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/cve-query-system.git
   cd cve-query-system
   ```

2. Install the required Python packages:
   ```bash
   pip install -r requirements.txt
   ```

3. Create a `.env` file in the root directory of your project and fill in the following keys:

   ```bash
   GROQ_API_KEY=your_groq_api_key
   PINECONE_API_KEY=your_pinecone_api_key
   PINECONE_ENVIRONMENT=your_pinecone_environment
   PINECONE_INDEX_NAME=your_pinecone_index_name
   PINECONE_HOST=your_pinecone_host
   SENTENCE_TRANSFORMER_MODEL=all-mpnet-base-v2
   LLM_MODEL=llama3-8b-8192
   LLM_TEMPERATURE=0.7
   K_NEIGHBORS=5
   FINAL_K=3
   ```

4. Start the Streamlit application:
   ```bash
   streamlit run streamlit_cve.py
   ```

### Usage

1. Enter a CVE identifier or a description into the input field.
2. The system retrieves the most relevant CVEs from the Pinecone vector database.
3. The retrieved CVEs are passed as context to the `llama3-8b-8192` model, which generates a human-readable response.

## Configuration

The application can be configured by adjusting the parameters in the `.env` file:

- **GROQ_API_KEY**: API key for accessing GroqCloud.
- **PINECONE_API_KEY**: API key for accessing Pinecone.
- **PINECONE_ENVIRONMENT**: The environment of your Pinecone project.
- **PINECONE_INDEX_NAME**: The name of your Pinecone index.
- **PINECONE_HOST**: The host URL for your Pinecone index.
- **SENTENCE_TRANSFORMER_MODEL**: The model used for generating embeddings, default is `all-mpnet-base-v2`.
- **LLM_MODEL**: The model used for generating responses, default is `llama3-8b-8192`.
- **LLM_TEMPERATURE**: The temperature setting for the LLM, affecting the creativity of responses.
- **K_NEIGHBORS**: The number of nearest neighbors to retrieve from Pinecone.
- **FINAL_K**: The final number of neighbors to pass to the LLM for generating responses.

## Deployment

This application can be deployed on GroqCloud for scalable and efficient compute. Make sure to configure your GroqCloud settings in the `.env` file.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgements

- [Streamlit](https://streamlit.io/)
- [Pinecone](https://www.pinecone.io/)
- [Sentence Transformers](https://www.sbert.net/)
- [GroqCloud](https://groq.com/)
- [LLaMA](https://ai.facebook.com/blog/large-language-model-llama/)

